1.正样本数(1703)：负样本数(1399) ≈ 1.22:1        
2.ECM反应的特征是红黄红的“汉堡”
3.频率在10KHZ以下的“汉堡”属于噪声
4.分析ECM利于找到延长核聚变反应的方法
5.train_batches(P:N = 1152:768)  test_batches(P:N = 403:92)
   →train_batches(P:N = 1152:1152)  test_batches(P:N = 551:247)
6.在2020.7.28，师兄发现有数据标错了，于是更新样本
   正样本数(1703)：负样本数(1399) ≈ 1.22:1   
   →.正样本数(1670)：负样本数(1432) ≈ 1.16:1   

  .train_batches(P:N = 1152:1152)  test_batches(P:N = 551:247)
   →train_batches(P:N = 1152:1152)  test_batches(P:N = 518:280)
7.在202.8.5 获得去噪数据，首先人工剔除了部分异常数据
	正样本数:负样本数 = 1300:1232 ≈ 1:1.056
		train_batches(2304)(1152:1152)		test_batches(228)(148:80)
8.在2020.8.16，自己手动筛选了正样本613张，负样本630张
		train(1000)	 pos:neg = 500:500 = 1:1
		test(243) 	 	 pos:neg = 113:130
9.在2020.12.26，写了一个数据清洗器，对样本进行了清洗，得
		正样本1409张：负样本1564张 ≈ 1：1.11
		→train_batches(P:N = 1200:1200)共2400张  ,  test_batches(P:N = 209:364)共573张
		计划使用数据增强，将train_batches增强到7200张。
10.在2021.2.3，为了提升模型测试的可信度，打算重新划分数据集
		正样本1409张：负样本1564张 ≈ 1：1.11 （不变）
		→train_batches(P:N = 1000:1000)共2000张  ,  test_batches(P:N = 409:564)共973张
		同时使用数据增强，将train_batches增强到8000张。

2019.11.28
第一次训练模型，训练集上精度几乎达到百分之百，然而测试集精度才百分之四十八，不到一半，基本是个废物模型，分析了原因，主要是因为过拟合，引起
过拟合的问题为①训练数据太少，加起来只有1920张图片 ②epochs过多

2019.11.29
第二次训练模型，将epochs从10减少到3，加入了keep_prob=0.5的Dropout，将全连接层的神经元个数从512增加到1024，模型精度从0.48上升至0.77，然而，还是
远未达到训练集的几乎百分百精度。

2019.12.5
从师兄那获得新数据，样本数量从正样本数(1555)：负样本数(860) ≈ 1.81:1 变成 正样本数(1703)：负样本数(1399) ≈ 1.22:1

2019.12.8
由于对数据的预处理算法出现问题（对正负样本的预处理不同，即对相同分辨率的图片采用不同的裁剪范围进行裁剪），
模型出现问题，无法使用（会根据对待测数据的裁剪范围产生一边倒的情况，若裁剪范围与正样本相同，则一边倒向正样本，反之亦然)，
分类器效果很差，偏向性很强，预估精度少于一半

2019.12.22
使用了迁移学习，将模型准确率从百分之77提升至百分之83，分类器的效果却出奇的好？？几乎全都能判断出来
使用了LSTM(基于keras)，将数据去色，训练速度出奇的快（主要原因还是神经元数量只有30个），也就几分钟，
测试集上精度达到百分之六十多，但分类器上效果还不错

2019.12.26~2020.2.8
尝试各种模型和参数结构，具体实现方式已整理成Excel文件。

2020.2.11
尝试使用自编码器给数据降噪，由于电脑硬件系统问题，无法训练大规模的自编码器，结果不太理想，在降噪的同时，图片丢失了许多重要的信息。

2020.11.6
使用色彩通道分析研究了cut blue算法，针对正样本图片右边出现的大块蓝色区域进行删减，达到去噪的效果。同时决定使用pytorch框架搭建模型。

2021.2.28
在分类精度达到96.4%（973张测试图片）后，打算进入定位工作，发现正样本中各张图片的时间坐标不一致，有0~5s的，有0~8s的，也有0~9s的，和0~7s的.
经过与曾富林师兄的讨论，决定从原始数据出发，利用某种算法根据原始数据的列数得到截止时间，然后标记在图片上，我再利用某个方法在算法中获取标记

